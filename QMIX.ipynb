{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding\\env\\cyj\\lib\\site-packages\\gym\\core.py:27: UserWarning: \u001b[33mWARN: Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001b[0m\n",
      "  \"Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate tuple (not \"int\") to tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-584245d14233>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;31m# 创建 QMIX 代理\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m \u001b[0mqmix_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQMIX\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;31m# 创建重播缓冲区\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-584245d14233>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# 根据参数决定RNN的输入维度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_action\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0minput_shape\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreuse_network\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_agents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate tuple (not \"int\") to tuple"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from network.base_net import RNN\n",
    "from network.qmix_net import QMixNet\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import env\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from env import CustomEnvironment \n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import random\n",
    "\n",
    "\n",
    "class QMIX:\n",
    "    def __init__(self, args):\n",
    "        self.n_actions = args.n_actions\n",
    "        self.n_agents = args.n_agents\n",
    "        self.state_shape = args.state_shape\n",
    "        self.obs_shape = args.obs_shape\n",
    "        input_shape = self.obs_shape\n",
    "        # 根据参数决定RNN的输入维度\n",
    "        if args.last_action:\n",
    "            input_shape += self.n_actions\n",
    "        if args.reuse_network:\n",
    "            input_shape += self.n_agents\n",
    "\n",
    "        # 神经网络\n",
    "        self.eval_rnn = RNN(input_shape, args)  # 每个agent选动作的网络\n",
    "        self.target_rnn = RNN(input_shape, args)\n",
    "        self.eval_qmix_net = QMixNet(args)  # 把agentsQ值加起来的网络\n",
    "        self.target_qmix_net = QMixNet(args)\n",
    "        self.args = args\n",
    "        if self.args.cuda:\n",
    "            self.eval_rnn.cuda()\n",
    "            self.target_rnn.cuda()\n",
    "            self.eval_qmix_net.cuda()\n",
    "            self.target_qmix_net.cuda()\n",
    "        self.model_dir = args.model_dir + '/' + args.alg + '/' + args.map\n",
    "        # 如果存在模型则加载模型\n",
    "        if self.args.load_model:\n",
    "            if os.path.exists(self.model_dir + '/rnn_net_params.pkl'):\n",
    "                path_rnn = self.model_dir + '/rnn_net_params.pkl'\n",
    "                path_qmix = self.model_dir + '/qmix_net_params.pkl'\n",
    "                map_location = 'cuda:0' if self.args.cuda else 'cpu'\n",
    "                self.eval_rnn.load_state_dict(torch.load(path_rnn, map_location=map_location))\n",
    "                self.eval_qmix_net.load_state_dict(torch.load(path_qmix, map_location=map_location))\n",
    "                print('Successfully load the model: {} and {}'.format(path_rnn, path_qmix))\n",
    "            else:\n",
    "                raise Exception(\"No model!\")\n",
    "\n",
    "        # 让target_net和eval_net的网络参数相同\n",
    "        self.target_rnn.load_state_dict(self.eval_rnn.state_dict())\n",
    "        self.target_qmix_net.load_state_dict(self.eval_qmix_net.state_dict())\n",
    "\n",
    "        self.eval_parameters = list(self.eval_qmix_net.parameters()) + list(self.eval_rnn.parameters())\n",
    "        if args.optimizer == \"RMS\":\n",
    "            self.optimizer = torch.optim.RMSprop(self.eval_parameters, lr=args.lr)\n",
    "\n",
    "        # 执行过程中，要为每个agent都维护一个eval_hidden\n",
    "        # 学习过程中，要为每个episode的每个agent都维护一个eval_hidden、target_hidden\n",
    "        self.eval_hidden = None\n",
    "        self.target_hidden = None\n",
    "        print('Init alg QMIX')\n",
    "\n",
    "    def learn(self, batch, max_episode_len, train_step, epsilon=None):  # train_step表示是第几次学习，用来控制更新target_net网络的参数\n",
    "        '''\n",
    "        在learn的时候，抽取到的数据是四维的，四个维度分别为 1——第几个episode 2——episode中第几个transition\n",
    "        3——第几个agent的数据 4——具体obs维度。因为在选动作时不仅需要输入当前的inputs，还要给神经网络输入hidden_state，\n",
    "        hidden_state和之前的经验相关，因此就不能随机抽取经验进行学习。所以这里一次抽取多个episode，然后一次给神经网络\n",
    "        传入每个episode的同一个位置的transition\n",
    "        '''\n",
    "        episode_num = batch['o'].shape[0]\n",
    "        self.init_hidden(episode_num)\n",
    "        for key in batch.keys():  # 把batch里的数据转化成tensor\n",
    "            if key == 'u':\n",
    "                batch[key] = torch.tensor(batch[key], dtype=torch.long)\n",
    "            else:\n",
    "                batch[key] = torch.tensor(batch[key], dtype=torch.float32)\n",
    "        s, s_next, u, r, avail_u, avail_u_next, terminated = batch['s'], batch['s_next'], batch['u'], \\\n",
    "                                                             batch['r'],  batch['avail_u'], batch['avail_u_next'],\\\n",
    "                                                             batch['terminated']\n",
    "        mask = 1 - batch[\"padded\"].float()  # 用来把那些填充的经验的TD-error置0，从而不让它们影响到学习\n",
    "\n",
    "        # 得到每个agent对应的Q值，维度为(episode个数, max_episode_len， n_agents， n_actions)\n",
    "        q_evals, q_targets = self.get_q_values(batch, max_episode_len)\n",
    "        if self.args.cuda:\n",
    "            s = s.cuda()\n",
    "            u = u.cuda()\n",
    "            r = r.cuda()\n",
    "            s_next = s_next.cuda()\n",
    "            terminated = terminated.cuda()\n",
    "            mask = mask.cuda()\n",
    "        # 取每个agent动作对应的Q值，并且把最后不需要的一维去掉，因为最后一维只有一个值了\n",
    "        q_evals = torch.gather(q_evals, dim=3, index=u).squeeze(3)\n",
    "\n",
    "        # 得到target_q\n",
    "        q_targets[avail_u_next == 0.0] = - 9999999\n",
    "        q_targets = q_targets.max(dim=3)[0]\n",
    "\n",
    "        q_total_eval = self.eval_qmix_net(q_evals, s)\n",
    "        q_total_target = self.target_qmix_net(q_targets, s_next)\n",
    "\n",
    "        targets = r + self.args.gamma * q_total_target * (1 - terminated)\n",
    "\n",
    "        td_error = (q_total_eval - targets.detach())\n",
    "        masked_td_error = mask * td_error  # 抹掉填充的经验的td_error\n",
    "\n",
    "        # 不能直接用mean，因为还有许多经验是没用的，所以要求和再比真实的经验数，才是真正的均值\n",
    "        loss = (masked_td_error ** 2).sum() / mask.sum()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.eval_parameters, self.args.grad_norm_clip)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if train_step > 0 and train_step % self.args.target_update_cycle == 0:\n",
    "            self.target_rnn.load_state_dict(self.eval_rnn.state_dict())\n",
    "            self.target_qmix_net.load_state_dict(self.eval_qmix_net.state_dict())\n",
    "\n",
    "    def _get_inputs(self, batch, transition_idx):\n",
    "        # 取出所有episode上该transition_idx的经验，u_onehot要取出所有，因为要用到上一条\n",
    "        obs, obs_next, u_onehot = batch['o'][:, transition_idx], \\\n",
    "                                  batch['o_next'][:, transition_idx], batch['u_onehot'][:]\n",
    "        episode_num = obs.shape[0]\n",
    "        inputs, inputs_next = [], []\n",
    "        inputs.append(obs)\n",
    "        inputs_next.append(obs_next)\n",
    "        # 给obs添加上一个动作、agent编号\n",
    "\n",
    "        if self.args.last_action:\n",
    "            if transition_idx == 0:  # 如果是第一条经验，就让前一个动作为0向量\n",
    "                inputs.append(torch.zeros_like(u_onehot[:, transition_idx]))\n",
    "            else:\n",
    "                inputs.append(u_onehot[:, transition_idx - 1])\n",
    "            inputs_next.append(u_onehot[:, transition_idx])\n",
    "        if self.args.reuse_network:\n",
    "            # 因为当前的obs三维的数据，每一维分别代表(episode编号，agent编号，obs维度)，直接在dim_1上添加对应的向量\n",
    "            # 即可，比如给agent_0后面加(1, 0, 0, 0, 0)，表示5个agent中的0号。而agent_0的数据正好在第0行，那么需要加的\n",
    "            # agent编号恰好就是一个单位矩阵，即对角线为1，其余为0\n",
    "            inputs.append(torch.eye(self.args.n_agents).unsqueeze(0).expand(episode_num, -1, -1))\n",
    "            inputs_next.append(torch.eye(self.args.n_agents).unsqueeze(0).expand(episode_num, -1, -1))\n",
    "        # 要把obs中的三个拼起来，并且要把episode_num个episode、self.args.n_agents个agent的数据拼成40条(40,96)的数据，\n",
    "        # 因为这里所有agent共享一个神经网络，每条数据中带上了自己的编号，所以还是自己的数据\n",
    "        inputs = torch.cat([x.reshape(episode_num * self.args.n_agents, -1) for x in inputs], dim=1)\n",
    "        inputs_next = torch.cat([x.reshape(episode_num * self.args.n_agents, -1) for x in inputs_next], dim=1)\n",
    "        return inputs, inputs_next\n",
    "\n",
    "    def get_q_values(self, batch, max_episode_len):\n",
    "        episode_num = batch['o'].shape[0]\n",
    "        q_evals, q_targets = [], []\n",
    "        for transition_idx in range(max_episode_len):\n",
    "            inputs, inputs_next = self._get_inputs(batch, transition_idx)  # 给obs加last_action、agent_id\n",
    "            if self.args.cuda:\n",
    "                inputs = inputs.cuda()\n",
    "                inputs_next = inputs_next.cuda()\n",
    "                self.eval_hidden = self.eval_hidden.cuda()\n",
    "                self.target_hidden = self.target_hidden.cuda()\n",
    "            q_eval, self.eval_hidden = self.eval_rnn(inputs, self.eval_hidden)  # inputs维度为(40,96)，得到的q_eval维度为(40,n_actions)\n",
    "            q_target, self.target_hidden = self.target_rnn(inputs_next, self.target_hidden)\n",
    "\n",
    "            # 把q_eval维度重新变回(8, 5,n_actions)\n",
    "            q_eval = q_eval.view(episode_num, self.n_agents, -1)\n",
    "            q_target = q_target.view(episode_num, self.n_agents, -1)\n",
    "            q_evals.append(q_eval)\n",
    "            q_targets.append(q_target)\n",
    "        # 得的q_eval和q_target是一个列表，列表里装着max_episode_len个数组，数组的的维度是(episode个数, n_agents，n_actions)\n",
    "        # 把该列表转化成(episode个数, max_episode_len， n_agents，n_actions)的数组\n",
    "        q_evals = torch.stack(q_evals, dim=1)\n",
    "        q_targets = torch.stack(q_targets, dim=1)\n",
    "        return q_evals, q_targets\n",
    "\n",
    "    def init_hidden(self, episode_num):\n",
    "        # 为每个episode中的每个agent都初始化一个eval_hidden、target_hidden\n",
    "        self.eval_hidden = torch.zeros((episode_num, self.n_agents, self.args.rnn_hidden_dim))\n",
    "        self.target_hidden = torch.zeros((episode_num, self.n_agents, self.args.rnn_hidden_dim))\n",
    "\n",
    "    def save_model(self, train_step):\n",
    "        num = str(train_step // self.args.save_cycle)\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            os.makedirs(self.model_dir)\n",
    "        torch.save(self.eval_qmix_net.state_dict(), self.model_dir + '/' + num + '_qmix_net_params.pkl')\n",
    "        torch.save(self.eval_rnn.state_dict(),  self.model_dir + '/' + num + '_rnn_net_params.pkl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiAgentReplayBuffer:\n",
    "    def __init__(self, capacity, num_agents):\n",
    "        self.capacity = capacity\n",
    "        self.num_agents = num_agents\n",
    "        self.buffers = {agent: collections.deque(maxlen=capacity) for agent in range(num_agents)}\n",
    "\n",
    "    def add(self, states, actions, rewards, next_states, dones):\n",
    "        for agent in range(self.num_agents):\n",
    "            self.buffers[agent].append((states[agent], actions[agent], rewards[agent], next_states[agent], dones[agent]))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        samples = {agent: random.sample(self.buffers[agent], batch_size) for agent in range(self.num_agents)}\n",
    "        states = {agent: [] for agent in range(self.num_agents)}\n",
    "        actions, rewards, next_states, dones = [], [], [], []\n",
    "\n",
    "        for agent in range(self.num_agents):\n",
    "            agent_samples = samples[agent]\n",
    "            for sample in agent_samples:\n",
    "                state, action, reward, next_state, done = sample\n",
    "                states[agent].append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                next_states.append(next_state)\n",
    "                dones.append(done)\n",
    "\n",
    "        return {agent: np.array(states[agent]) for agent in range(self.num_agents)}, np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "\n",
    "    def size(self):\n",
    "        return min(len(self.buffers[agent]) for agent in range(self.num_agents))\n",
    "\n",
    "\n",
    "    # 创建自定义环境的实例\n",
    "env = CustomEnvironment()\n",
    "\n",
    "def train_qmix(qmix_agent, env, num_episodes, replay_buffer, minimal_size, batch_size):\n",
    "    returns = []  # 用于存储每个回合的回报\n",
    "    max_q_value_list = []  # 用于存储每个回合的最大Q值\n",
    "\n",
    "    for i in range(10):\n",
    "        with tqdm(total=int(num_episodes / 10),\n",
    "                  desc='Iteration %d' % i) as pbar:\n",
    "            for i_episode in range(int(num_episodes / 10)):\n",
    "                episode_return = 0\n",
    "                obs = env.reset()\n",
    "                done = False\n",
    "\n",
    "                while not done:\n",
    "                    # 根据QMIX算法选择动作，并与环境交互\n",
    "                    actions = qmix_agent.select_actions(obs)  # 选择每个代理的动作\n",
    "                    next_obs, rewards, done, _ = env.step(actions)  # 与环境交互\n",
    "                    episode_return += sum(rewards)\n",
    "\n",
    "                    # 将经验存储到重播缓冲区\n",
    "                    replay_buffer.add(obs, actions, rewards, next_obs, done)\n",
    "\n",
    "                    # 如果重播缓冲区已达到最小大小，进行训练\n",
    "                    if replay_buffer.size() > minimal_size:\n",
    "                        batch = replay_buffer.sample(batch_size)\n",
    "                        transition_dict = {\n",
    "                            'states': batch[0],\n",
    "                            'actions': batch[1],\n",
    "                            'rewards': batch[2],\n",
    "                            'next_states': batch[3],\n",
    "                            'dones': batch[4]\n",
    "                        }\n",
    "\n",
    "                        # 根据QMIX算法更新代理的Q值\n",
    "                        qmix_agent.update(transition_dict)\n",
    "\n",
    "                        # 计算当前状态的 Q 值并将其添加到 max_q_value_list\n",
    "                        current_q_values = qmix_agent.compute_q_values(obs)\n",
    "                        max_q_value_list.append(max(current_q_values))\n",
    "\n",
    "                    obs = next_obs  # 更新状态为下一个状态\n",
    "\n",
    "                returns.append(episode_return)\n",
    "                if (i_episode + 1) % 10 == 0:\n",
    "                    pbar.set_postfix({\n",
    "                        'episode': '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                        'return': '%.3f' % np.mean(returns[-10:])\n",
    "                    })\n",
    "                pbar.update(1)\n",
    "\n",
    "    return returns, max_q_value_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_results(returns, q_values):\n",
    "    # 用于绘制回报和Q值与迭代次数的图表\n",
    "    # 使用matplotlib或其他绘图库创建所需的图表\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(returns)\n",
    "    plt.title('回报 vs. 回合')\n",
    "    plt.xlabel('回合')\n",
    "    plt.ylabel('回报')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(q_values)\n",
    "    plt.title('Q值 vs. 回合')\n",
    "    plt.xlabel('回合')\n",
    "    plt.ylabel('Q值')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        # 定义 QMIX 训练参数\n",
    "        self.n_actions = 24  # 动作空间大小\n",
    "        self.n_agents = 4  # 代理数量\n",
    "        self.state_shape = (96,)  # 状态空间形状\n",
    "        self.obs_shape = (48,)  # 观测空间形状\n",
    "        self.last_action = True  # 是否考虑上一个动作\n",
    "        self.reuse_network = True  # 是否共享神经网络\n",
    "        self.cuda = True  # 是否使用 GPU\n",
    "        self.load_model = False  # 是否加载预训练模型\n",
    "        self.model_dir = 'models'  # 模型保存路径\n",
    "        self.lr = 0.001  # 学习率\n",
    "        self.optimizer = 'RMS'  # 优化器选择\n",
    "        self.gamma = 0.99  # 折扣因子\n",
    "        self.rnn_hidden_dim = 64  # RNN 隐藏层维度\n",
    "        self.grad_norm_clip = 10.0  # 梯度裁剪阈值\n",
    "        self.target_update_cycle = 10  # 目标网络更新周期\n",
    "        self.save_cycle = 1000  # 模型保存周期\n",
    "\n",
    "# 创建 args 对象并初始化参数\n",
    "args = Args()\n",
    "\n",
    "# 创建环境实例\n",
    "env = CustomEnvironment()\n",
    "\n",
    "# 定义训练参数\n",
    "num_episodes = 1000\n",
    "batch_size = 64\n",
    "minimal_size = 1000\n",
    "\n",
    "# 创建 QMIX 代理\n",
    "qmix_agent = QMIX(args)\n",
    "\n",
    "# 创建重播缓冲区\n",
    "replay_buffer = MultiAgentReplayBuffer(capacity=10000, num_agents=args.n_agents)\n",
    "\n",
    "# 调用 train_qmix 函数进行训练\n",
    "returns, max_q_value_list = train_qmix(qmix_agent, env, num_episodes, replay_buffer, minimal_size, batch_size)\n",
    "\n",
    "# 绘制结果图表\n",
    "plot_results(returns, max_q_value_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
