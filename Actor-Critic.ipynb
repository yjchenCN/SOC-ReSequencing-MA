{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import env\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Actor网络（RNN）\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_shape, args):\n",
    "        super(RNN, self).__init__()\n",
    "        self.args = args\n",
    "        self.rnn = nn.GRU(input_size=input_shape, hidden_size=args.rnn_hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(args.rnn_hidden_dim, args.n_actions)\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        x, h = self.rnn(x, hidden_state)\n",
    "        q = self.fc1(x)\n",
    "        return q, h\n",
    "\n",
    "# Critic网络（QMIX）\n",
    "class QMixNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(QMixNet, self).__init__()\n",
    "        self.args = args\n",
    "        # 根据您的需求定义QMIX网络\n",
    "        # 示例：一些全连接层\n",
    "        self.fc1 = nn.Linear(args.state_shape, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        inputs = torch.cat([states, actions], dim=1)\n",
    "        x = torch.relu(self.fc1(inputs))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        q_total = self.fc3(x)\n",
    "        return q_total\n",
    "    \n",
    "\n",
    "\n",
    "class QMIX:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.n_agents = args.n_agents\n",
    "        self.n_actions = args.n_actions\n",
    "        self.state_shape = args.state_shape\n",
    "        self.obs_shape = args.obs_shape\n",
    "\n",
    "        # Actor Networks for each agent\n",
    "        self.actors = [RNN(args.obs_shape, args) for _ in range(self.n_agents)]\n",
    "\n",
    "        # Critic Network\n",
    "        self.critic = QMixNet(args)\n",
    "\n",
    "        # Optimizers\n",
    "        self.actor_optimizers = [optim.Adam(actor.parameters(), lr=args.lr) for actor in self.actors]\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=args.lr)\n",
    "\n",
    "        # CUDA\n",
    "        if args.use_cuda:\n",
    "            self.critic.cuda()\n",
    "            for actor in self.actors:\n",
    "                actor.cuda()\n",
    "\n",
    "    def select_action(self, obs, hidden_states, exploration=False):\n",
    "        actions = []\n",
    "        next_hidden_states = []\n",
    "        for agent_id, actor in enumerate(self.actors):\n",
    "            obs_tensor = torch.tensor(obs[agent_id], dtype=torch.float32).unsqueeze(0)\n",
    "            if self.args.use_cuda:\n",
    "                obs_tensor = obs_tensor.cuda()\n",
    "            q_values, next_hidden_state = actor(obs_tensor, hidden_states[agent_id])\n",
    "            next_hidden_states.append(next_hidden_state)\n",
    "\n",
    "            if exploration:\n",
    "                action = np.random.choice(self.n_actions)\n",
    "            else:\n",
    "                action = q_values.max(dim=-1)[1].cpu().numpy().item()\n",
    "            actions.append(action)\n",
    "        return actions, next_hidden_states\n",
    "\n",
    "    # Training method and other methods will be added next\n",
    "\n",
    "    def train(self, transitions, hidden_states):\n",
    "        # 转换数据格式\n",
    "        obs, actions, rewards, next_obs, done = zip(*transitions)\n",
    "\n",
    "        # 转换成张量\n",
    "        obs = torch.tensor(obs, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).view(-1, 1)\n",
    "        next_obs = torch.tensor(next_obs, dtype=torch.float32)\n",
    "        done = torch.tensor(done, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        if self.args.use_cuda:\n",
    "            obs = obs.cuda()\n",
    "            actions = actions.cuda()\n",
    "            rewards = rewards.cuda()\n",
    "            next_obs = next_obs.cuda()\n",
    "            done = done.cuda()\n",
    "\n",
    "        # 计算当前状态的Q值和下一个状态的Q值\n",
    "        current_Q_values, _ = zip(*[actor(obs[:, agent_id, :], hidden_states[agent_id]) for agent_id, actor in enumerate(self.actors)])\n",
    "        current_Q_values = torch.stack(current_Q_values, dim=1)\n",
    "\n",
    "        # 选择动作的Q值\n",
    "        current_Q_values = current_Q_values.gather(dim=2, index=actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # 计算目标Q值\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_hidden_states = self.select_action(next_obs, hidden_states, exploration=False)\n",
    "            next_actions = torch.tensor(next_actions, dtype=torch.long).unsqueeze(-1)\n",
    "            next_Q_values, _ = zip(*[actor(next_obs[:, agent_id, :], next_hidden_states[agent_id]) for agent_id, actor in enumerate(self.actors)])\n",
    "            next_Q_values = torch.stack(next_Q_values, dim=1)\n",
    "            next_Q_values = next_Q_values.max(dim=2)[0]\n",
    "\n",
    "        # 计算总的Q值\n",
    "        total_Q_values = self.critic(obs.view(-1, self.state_shape), current_Q_values.view(-1, self.n_agents))\n",
    "        total_Q_values = total_Q_values.view(-1, 1)\n",
    "\n",
    "        # 计算目标总Q值\n",
    "        target_total_Q_values = self.critic(next_obs.view(-1, self.state_shape), next_Q_values.view(-1, self.n_agents))\n",
    "        target_total_Q_values = rewards + self.args.gamma * (1 - done) * target_total_Q_values.view(-1, 1)\n",
    "\n",
    "        # 计算损失并优化\n",
    "        loss = torch.mean((total_Q_values - target_total_Q_values.detach()) ** 2)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # 更新Actor网络\n",
    "        for actor, optimizer in zip(self.actors, self.actor_optimizers):\n",
    "            optimizer.zero_grad()\n",
    "            actor_loss = -self.critic(obs.view(-1, self.state_shape), current_Q_values.view(-1, self.n_agents)).mean()\n",
    "            actor_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save({'actors_state_dict': [actor.state_dict() for actor in self.actors],\n",
    "                    'critic_state_dict': self.critic.state_dict()}, path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        for actor, state_dict in zip(self.actors, checkpoint['actors_state_dict']):\n",
    "            actor.load_state_dict(state_dict)\n",
    "        self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "    \n",
    "\n",
    "class Args:\n",
    "    # 环境参数\n",
    "    n_actions = 24  # 假设有7种可能的动作\n",
    "    n_agents = 4   # 您环境中的智能体数量\n",
    "    state_shape = 5  # 状态空间的维度，您环境中的state维度\n",
    "    obs_shape = 5   # 观测空间的维度，与状态空间维度相同\n",
    "    rnn_hidden_dim = 64  # RNN隐藏层的维度\n",
    "    lr = 0.01  # 学习率\n",
    "    gamma = 0.95  # 折扣因子\n",
    "    use_cuda = torch.cuda.is_available()  # 是否使用CUDA\n",
    "    # ... 可以添加更多您需要的参数 ...\n",
    "\n",
    "\n",
    "\n",
    "# 创建环境和QMIX实例\n",
    "env = env.CustomEnvironment()\n",
    "args = Args()\n",
    "qmix_agent = QMIX(args)\n",
    "\n",
    "# 训练参数\n",
    "n_episodes = 1000\n",
    "rewards_history = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    # 初始化环境和隐藏状态\n",
    "    obs = env.reset()\n",
    "    hidden_states = [torch.zeros(args.rnn_hidden_dim) for _ in range(args.n_agents)]\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # 选择动作并执行环境步骤\n",
    "        actions, next_hidden_states = qmix_agent.select_action(obs, hidden_states, exploration=True)\n",
    "        next_obs, rewards, dones, _ = env.step(actions)\n",
    "        total_reward += sum(rewards.values())\n",
    "\n",
    "        # 收集训练数据\n",
    "        transitions = [(obs[agent], actions[agent], rewards[agent], next_obs[agent], dones[agent]) for agent in range(args.n_agents)]\n",
    "        loss = qmix_agent.train(transitions, hidden_states)\n",
    "\n",
    "        obs = next_obs\n",
    "        hidden_states = next_hidden_states\n",
    "        done = all(dones.values())\n",
    "\n",
    "    rewards_history.append(total_reward)\n",
    "    print(f\"Episode {episode}, Total Reward: {total_reward}, Loss: {loss}\")\n",
    "\n",
    "# 绘制奖励随迭代次数变化的折线图\n",
    "plt.plot(rewards_history)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward over Episodes')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
